import time
import json
import pandas as pd
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import random
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Liste des banques et villes
BANKS = ["Al Barid Bank","ATTIJARIWAFA BANK","BANK OF AFRICA","BMCI","SOCI√âT√â G√âN√âRALE MAROC","CIH BANK","CREDIT DU MAROC","BANK AL YOUSR","BANK ASSAFA","UMNIA BANK","BANQUE CENTRALE POPULAIRE"]

CITIES = ["Casablanca","Rabat","Fes","Tanger","Agadir","Marrakech","sale","Meknes","AL hoceima","Mohammadia","Settat","Arfoud","Nador","Essaouira","Errachidia","Berkane","Asila","Tetouan","Taroudant"]

BASE_URL = "https://www.google.com/maps/search/"
MIN_REVIEWS_PER_BRANCH = 100
MAX_AGENCIES_PER_BANK = 80   


def random_delay(min_seconds=1, max_seconds=3):
    """Attendre un temps al√©atoire pour simuler un comportement humain"""
    delay = random.uniform(min_seconds, max_seconds)
    time.sleep(delay)
    return delay

def handle_consent_popup(page):
    """G√®re les popups de consentement de cookies"""
    try:
        # Diff√©rents s√©lecteurs possibles pour les boutons de consentement
        consent_buttons = [
            'button:has-text("Tout accepter")',
            'button:has-text("J\'accepte")',
            'button:has-text("Accepter tout")',
            'button:has-text("Accept all")',
            'button:has-text("I agree")',
            'button:has-text("Agree")'
        ]
        
        for selector in consent_buttons:
            if page.locator(selector).count() > 0:
                logger.info(f"Popup de consentement d√©tect√©. Clique sur '{selector}'")
                page.locator(selector).click()
                random_delay(2, 4)
                return True
                
        return False
    except Exception as e:
        logger.warning(f"Erreur lors de la gestion du popup de consentement: {e}")
        return False

def scroller_banques(page):
    """Scrolle la liste des r√©sultats pour charger plus d'agences"""
    try:
        # Utilisation d'un s√©lecteur CSS plus g√©n√©rique
        results_container = page.locator('div[role="feed"]')
        if results_container.count() > 0:
            logger.info("Scroll des r√©sultats pour charger plus d'agences...")
            for i in range(5):  # R√©duit √† 5 scrolls
                page.mouse.wheel(0, 1000)
                delay = random_delay(1.5, 3)
                logger.info(f"Scroll {i+1}/5 - Attente de {delay:.1f}s")
            return True
        else:
            logger.warning("Conteneur de r√©sultats non trouv√©")
            return False
    except Exception as e:
        logger.error(f"Erreur lors du scroll des r√©sultats: {e}")
        return False

# Dans la fonction cliquer_sur_avis, remplacez le code par cette version am√©lior√©e :

def cliquer_sur_avis(page):
    """Clique sur l'onglet Avis d'une agence bancaire"""
    try:
        # V√©rifier si nous sommes d√©j√† sur l'onglet des avis
        # Si des √©l√©ments d'avis sont visibles, on est d√©j√† sur le bon onglet
        if page.locator('div[data-review-id]').count() > 0:
            logger.info("D√©j√† sur l'onglet des avis")
            return True
            
        # Essayer d'abord avec un s√©lecteur plus sp√©cifique pour le tab "Avis"
        reviews_tab = page.locator('button[role="tab"][aria-label*="Avis"]').first
        if reviews_tab.count() > 0:
            logger.info("Clic sur l'onglet Avis (tab)")
            reviews_tab.click()
            random_delay(2, 4)
            return True
        
        # Essayer avec le bouton qui affiche le nombre d'avis
        reviews_count_button = page.locator('button.GQjSyb:has-text("avis")').first
        if reviews_count_button.count() > 0:
            logger.info("Clic sur le bouton de nombre d'avis")
            reviews_count_button.click()
            random_delay(2, 4)
            return True
            
        # Essayer avec "Plus d'avis"
        more_reviews = page.locator('button[aria-label*="Plus d\'avis"]').first
        if more_reviews.count() > 0:
            logger.info("Clic sur 'Plus d'avis'")
            more_reviews.click()
            random_delay(2, 4)
            return True
            
        # En dernier recours, essayer de cliquer sur la note moyenne
        rating_element = page.locator('button.fontDisplayLarge').first
        if rating_element.count() > 0:
            logger.info("Clic sur l'√©l√©ment de notation")
            rating_element.click()
            random_delay(2, 4)
            return True
            
        logger.warning("Aucun √©l√©ment d'avis n'a pu √™tre trouv√© ou cliqu√©")
        return False
    except Exception as e:
        logger.error(f"Erreur lors du clic sur l'onglet Avis: {e}")
        return False

def wait_for_reviews_to_load(page):
    """Attend que les avis se chargent"""
    try:
        logger.info("Attente du chargement des avis...")
        review_items = page.locator('div[data-review-id]')
        # Attendre jusqu'√† 10 secondes que les avis apparaissent
        review_items.first.wait_for(timeout=10000)
        count = review_items.count()
        logger.info(f"{count} avis charg√©s initialement")
        return count > 0
    except Exception as e:
        logger.warning(f"Erreur ou timeout lors de l'attente des avis: {e}")
        return False

def charger_et_extraire_avis(page):
    """Charge et extrait les avis sans duplication"""
    if not wait_for_reviews_to_load(page):
        return []
    
    try:
        # Scrolle pour charger plus d'avis
        reviews_container = page.locator('div[role="feed"]')
        if reviews_container.count() > 0:
            logger.info("Scroll pour charger plus d'avis...")
            for i in range(8):
                page.mouse.wheel(0, 1000)
                random_delay(1, 2)
                
        # D√©velopper tous les avis (cliquer sur "Lire la suite")
        more_buttons = page.locator('button.w8nwRe.kyuRq, button[jsaction*="pane.review.expandReview"]')
        count = more_buttons.count()
        if count > 0:
            logger.info(f"D√©veloppement de {count} boutons 'plus'...")
            for i in range(count):  # Augmenter pour cliquer sur tous les boutons "lire la suite"
                try:
                    # Essayer de cliquer sur chaque bouton visible
                    if more_buttons.nth(i).is_visible():
                        more_buttons.nth(i).click()
                        random_delay(0.5, 1)
                except Exception as e:
                    logger.warning(f"Erreur en cliquant sur le bouton 'plus' #{i}: {e}")
                    
        # Attendre un peu que tous les avis se d√©veloppent
        random_delay(2, 3)
                
        # Maintenant, on extrait les avis depuis la page compl√®te
        soup = BeautifulSoup(page.content(), "html.parser")
        
        # Essai avec plusieurs s√©lecteurs possibles
        review_containers = soup.select('div[data-review-id]') or soup.select('.jftiEf')
        
        if not review_containers:
            logger.warning("Aucun avis trouv√© avec les s√©lecteurs utilis√©s")
            return []
            
        logger.info(f"Extraction de {len(review_containers)} avis...")
        
        extracted_reviews = []
        seen_reviews = set()  # Ensemble pour suivre les avis d√©j√† vus
        
        for review in review_containers:
            try:
                # Texte de l'avis
                review_text_element = (
                    review.select_one('.wiI7pd') or 
                    review.select_one('.MyEned') or
                    review.select_one('span[jsan*="reviews.snippet.text"]')
                )

                review_text = review_text_element.text.strip() if review_text_element else "Pas de texte"
                
                # Note (√©toiles)
                rating_element = (
                    review.select_one('span[aria-label*="√©toile"]') or
                    review.select_one('span[aria-label*="star"]') or
                    review.select_one('.kvMYJc')
                )
                rating = rating_element["aria-label"] if rating_element and rating_element.has_attr("aria-label") else "Pas de note"
                
                # Date
                date_element = (
                    review.select_one('.rsqaWe') or
                    review.select_one('span[jsan*="reviews.snippet.publish_date"]')
                )
                date = date_element.text.strip() if date_element else "Pas de date"
                
                # Cr√©er un hachage unique pour cet avis
                review_hash = hash(f"{review_text}|{rating}|{date}")
                
                # V√©rifier si cet avis a d√©j√† √©t√© vu
                if review_hash not in seen_reviews:
                    seen_reviews.add(review_hash)
                    extracted_reviews.append({
                        "Review": review_text,
                        "Rating": rating,
                        "Date": date
                    })
            except Exception as e:
                logger.warning(f"Erreur lors de l'extraction d'un avis: {e}")
        
        logger.info(f"{len(extracted_reviews)} avis uniques extraits avec succ√®s")
        return extracted_reviews
    except Exception as e:
        logger.error(f"Erreur g√©n√©rale lors de l'extraction des avis: {e}")
        return []
def get_location_details(page, soup):
    """Extrait les d√©tails de l'agence bancaire"""
    try:
        # Nom de l'agence
        branch_elements = [
            soup.select_one('h1.DUwDvf'),
            soup.select_one('h1[jsan*="header-title"]'),
            soup.select_one('h1.fontHeadlineLarge')
        ]
        branch = next((el.text.strip() for el in branch_elements if el), "Nom inconnu")
        
        # Adresse
        address_elements = [
            soup.select_one('button[data-item-id="address"]'),
            soup.select_one('button[jsaction*="pane.address.click"]'),
            soup.select_one('.Io6YTe')
        ]
        location = next((el.text.strip() for el in address_elements if el), "Adresse inconnue")
        
        return branch, location
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des d√©tails de l'agence: {e}")
        return "Nom inconnu", "Adresse inconnue"

# 1. Fonction alternative pour acc√©der aux d√©tails de l'agence sans cliquer sur l'onglet Avis
def get_agency_reviews_direct(page, bank, city, agency_name=None):
    """Approche alternative pour obtenir les avis sans utiliser les onglets"""
    try:
        # Construire une URL directe pour les avis Google Maps
        search_term = f"{bank} {agency_name or ''} {city} Maroc avis"
        direct_url = f"https://www.google.com/search?q={search_term.replace(' ', '+')}"
        
        logger.info(f"Tentative d'acc√®s direct aux avis via: {direct_url}")
        page.goto(direct_url, timeout=30000)
        random_delay(3, 5)
        
        # G√©rer le popup de consentement
        handle_consent_popup(page)
        
        # Chercher et cliquer sur le lien "Avis Google"
        google_reviews_link = page.locator('a:has-text("Avis Google")').first
        if google_reviews_link.count() > 0:
            logger.info("Clic sur 'Avis Google'")
            google_reviews_link.click()
            random_delay(3, 5)
            
            # V√©rifier si des avis sont charg√©s
            if wait_for_reviews_to_load(page):
                return True
        
        return False
    except Exception as e:
        logger.error(f"Erreur lors de l'acc√®s direct aux avis: {e}")
        return False

# 2. Am√©liorez la fonction wait_for_reviews_to_load pour √™tre plus flexible
def wait_for_reviews_to_load(page):
    """Attend que les avis se chargent avec diff√©rents s√©lecteurs possibles"""
    try:
        logger.info("Attente du chargement des avis...")
        
        # Essayer diff√©rents s√©lecteurs pour les avis
        selectors = [
            'div[data-review-id]',  # S√©lecteur standard pour les avis
            '.gws-localreviews__google-review',  # Alternative sur la page de recherche Google
            '.WMbnJf',  # Alternative possible
            'div[jsaction*="reviewerLink"]'  # Alternative bas√©e sur le jsaction
        ]
        
        for selector in selectors:
            try:
                review_items = page.locator(selector)
                # Attendre jusqu'√† 5 secondes
                review_items.first.wait_for(timeout=5000)
                count = review_items.count()
                if count > 0:
                    logger.info(f"{count} avis charg√©s avec le s√©lecteur '{selector}'")
                    return True
            except Exception:
                continue
                
        logger.warning("Aucun avis trouv√© avec les s√©lecteurs disponibles")
        return False
    except Exception as e:
        logger.warning(f"Erreur lors de l'attente des avis: {e}")
        return False

# 3. Modifiez get_reviews_for_bank pour utiliser l'approche alternative si n√©cessaire
def get_reviews_for_bank(bank, city, page):
    """R√©cup√®re les avis pour une banque dans une ville avec m√©thode de secours"""
    search_url = BASE_URL + f"{bank} {city} Maroc".replace(" ", "+")
    logger.info(f"Navigation vers: {search_url}")
    
    try:
        page.goto(search_url, timeout=60000)
        random_delay(3, 5)
        
        # G√©rer le popup de consentement si pr√©sent
        handle_consent_popup(page)
        
        # Scroller pour charger plus d'agences
        scroller_banques(page)
        
        # S√©lecteur plus g√©n√©rique pour les liens d'agences
        agency_links = page.locator('a[href*="/maps/place/"]')
        count = agency_links.count()
        
        if count == 0:
            logger.warning(f"Aucune agence trouv√©e pour {bank} √† {city}")
            return []
            
        logger.info(f"{count} agences trouv√©es pour {bank} √† {city}")
        
        results = []
        agencies_to_check = min(count, MAX_AGENCIES_PER_BANK)
        
        for i in range(agencies_to_check):
            try:
                logger.info(f"Traitement de l'agence {i+1}/{agencies_to_check}")
                
                # Extraire le nom de l'agence si possible
                agency_element = agency_links.nth(i)
                agency_name = agency_element.inner_text() if agency_element.count() > 0 else None
                
                # Approche principale: cliquer sur l'agence
                agency_element.click()
                random_delay(3, 5)
                
                # Cliquer sur l'onglet Avis
                avis_clicked = cliquer_sur_avis(page)
                
                # Si √©chec avec l'approche principale, essayer l'approche alternative
                if not avis_clicked:
                    logger.warning(f"√âchec de l'approche principale pour l'agence {i+1}, essai avec approche alternative")
                    page.go_back()
                    random_delay(2, 3)
                    
                    if get_agency_reviews_direct(page, bank, city, agency_name):
                        logger.info("Approche alternative r√©ussie pour acc√©der aux avis")
                        avis_clicked = True
                    else:
                        logger.warning(f"Impossible d'acc√©der aux avis pour l'agence {i+1}, passage √† la suivante")
                        page.goto(search_url, timeout=30000)
                        random_delay(3, 5)
                        scroller_banques(page)
                        continue
                
                # Charger et extraire les avis
                extracted_reviews = charger_et_extraire_avis(page)
                
                if extracted_reviews:
                    # Extraire les d√©tails de l'agence
                    soup = BeautifulSoup(page.content(), "html.parser")
                    branch, location = get_location_details(page, soup)
                    
                    # Ajouter les d√©tails √† chaque avis
                    for review in extracted_reviews:
                        results.append({
                            "Bank": bank,
                            "City": city,
                            "Branch": branch,
                            "Location": location,
                            **review
                        })
                    
                    logger.info(f"{len(extracted_reviews)} avis ajout√©s pour {branch}")
                else:
                    logger.warning(f"Aucun avis extrait pour l'agence {i+1}")
                
                # Retour aux r√©sultats
                page.goto(search_url, timeout=30000)
                random_delay(3, 5)
                scroller_banques(page)
                
            except Exception as e:
                logger.error(f"Erreur avec l'agence {i+1}: {e}")
                # Revenir aux r√©sultats
                try:
                    page.goto(search_url, timeout=30000)
                    random_delay(3, 5)
                    scroller_banques(page)
                except Exception as nav_error:
                    logger.error(f"Erreur de navigation: {nav_error}")
        
        return results
    except Exception as e:
        logger.error(f"Erreur g√©n√©rale pour {bank} √† {city}: {e}")
        return []

def scrape_reviews():
    """Fonction principale de scraping"""
    all_reviews = []
    
    # R√©duire le nombre de villes et banques pour les tests
    test_cities = CITIES  # Seulement les 3 premi√®res villes
    test_banks = BANKS   # Seulement les 3 premi√®res banques
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = context.new_page()
        
        # Param√®tres de page pour ressembler davantage √† un utilisateur humain
        page.set_viewport_size({"width": 1280, "height": 800})
        
        try:
            for city in test_cities:
                for bank in test_banks:
                    logger.info(f"üîç Scraping {bank} √† {city}")
                    reviews = get_reviews_for_bank(bank, city, page)
                    all_reviews.extend(reviews)
                    
                    # Sauvegarder les r√©sultats interm√©diaires
                    if reviews:
                        logger.info(f"üíæ Sauvegarde interm√©diaire: {len(all_reviews)} avis au total")
                        try:
                            with open(f"avis_intermediaire_1{len(all_reviews)}.json", "w", encoding="utf-8") as json_file:
                                json.dump(all_reviews, json_file, ensure_ascii=False, indent=4)
                        except Exception as save_error:
                            logger.error(f"Erreur lors de la sauvegarde interm√©diaire: {save_error}")
                    
                    # Pause entre les banques
                    random_delay(5, 10)
        except Exception as e:
            logger.error(f"Erreur g√©n√©rale de scraping: {e}")
        finally:
            browser.close()
            
    return all_reviews

# Ex√©cuter le scraping
def main() :
    logger.info("üöÄ D√©marrage du scraping des avis bancaires")
    reviews = scrape_reviews()
    
    if reviews:
        logger.info(f"‚úÖ Scraping termin√© ! {len(reviews)} avis collect√©s.")
        
        # Sauvegarde JSON
        try:
            with open("avies.json", "w", encoding="utf-8") as json_file:
                json.dump(reviews, json_file, ensure_ascii=False, indent=4)
            logger.info("üíæ Fichier JSON sauvegard√© avec succ√®s")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde JSON: {e}")
        
        # Sauvegarde CSV
        try:
            pd.DataFrame(reviews).to_csv("avies.csv", index=False, encoding="utf-8")
            logger.info("üíæ Fichier CSV sauvegard√© avec succ√®s")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde CSV: {e}")
    else:
        logger.error("‚ùå Aucun avis collect√©!")
        
if __name__ == "__main__":
    main()